{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam.base_cam import BaseCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GradRAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers,\n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradRAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        return np.mean(grads, axis=(2, 3))\n",
    "\n",
    "    def forward(self,\n",
    "                input_tensor: torch.Tensor,\n",
    "                targets: List[torch.nn.Module],\n",
    "                eigen_smooth: bool = False) -> np.ndarray:\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "\n",
    "        if self.compute_input_gradient:\n",
    "            input_tensor = torch.autograd.Variable(input_tensor,\n",
    "                                                   requires_grad=True)\n",
    "\n",
    "        self.outputs = outputs = self.activations_and_grads(input_tensor)\n",
    "        self.model.requires_grad_(True)\n",
    "\n",
    "        if self.uses_gradients:\n",
    "            self.model.zero_grad()\n",
    "            loss = targets[0](outputs)\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "        cam_per_layer = self.compute_cam_per_layer(input_tensor,\n",
    "                                                   targets,\n",
    "                                                   eigen_smooth)\n",
    "        return self.aggregate_multi_layers(cam_per_layer)\n",
    "\n",
    "\n",
    "class LogProbOutputTarget:\n",
    "    def __init__(self, target_value):\n",
    "        self.target_value = target_value\n",
    "    def __call__(self, model_output):\n",
    "        log_probs = model_output.log_prob(self.target_value)\n",
    "        loss = -log_probs.mean() # loss is just negative log-likelihood of action targets\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('lift/depth84.hdf5', 'r') as f:\n",
    "    front_images = f['data']['demo_0']['obs']['frontview_image'][:]\n",
    "    agent_images = f['data']['demo_0']['obs']['agentview_image'][:]\n",
    "    side_images = f['data']['demo_0']['obs']['sideview_image'][:]\n",
    "    eyeinhand_images = f['data']['demo_0']['obs']['robot0_eye_in_hand_image'][:]\n",
    "    eef_poses = f['data']['demo_0']['obs']['robot0_eef_pos'][:]\n",
    "    eef_quats = f['data']['demo_0']['obs']['robot0_eef_quat'][:]\n",
    "    gripper_qposes = f['data']['demo_0']['obs']['robot0_gripper_qpos'][:]\n",
    "    actions = f['data']['demo_0']['actions'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models and run gradcam here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [512, 3, 3]\n",
    "image_latent_dim = 256\n",
    "action_dim = 7\n",
    "low_dim_input_dim = 3 + 4 + 2  # robot0_eef_pos + robot0_eef_quat + robot0_gripper_qpos\n",
    "mlp_hidden_dims = [1024, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.lift.baseline.resnet18_gmmmlp_view13rgb_model_ver1 import PiNetwork\n",
    "from models.lift.baseline.resnet18_gmmmlp_view13rgb_model_ver1 import Imageonly_Model\n",
    "\n",
    "model = PiNetwork(input_shape, image_latent_dim, action_dim, low_dim_input_dim, mlp_hidden_dims)\n",
    "model.to(device)\n",
    "model.float()\n",
    "\n",
    "# test load and rollout\n",
    "\n",
    "vision1_encoder_path = '/home/generalroboticslab/Desktop/gradcam-view-stitching/models/lift/baseline/models/bc_robomimic_ver1_vision1robot0_eye_in_hand_vision2frontview_anchors256_lr0.0001_seed101_model.pt'\n",
    "vision2_encoder_path = '/home/generalroboticslab/Desktop/gradcam-view-stitching/models/lift/baseline/models/bc_robomimic_ver1_vision1agentview_vision2sideview_anchors256_lr0.0001_seed101_model.pt'\n",
    "gmm_mlp_path = '/home/generalroboticslab/Desktop/gradcam-view-stitching/models/lift/baseline/models/bc_robomimic_ver1_vision1robot0_eye_in_hand_vision2frontview_anchors256_lr0.0001_seed101_model.pt'\n",
    "\n",
    "data1 = torch.load(vision1_encoder_path, map_location=device)  # data1 for task vision1_encoder\n",
    "data2 = torch.load(vision2_encoder_path, map_location=device)  # data2 for vision2_encoder\n",
    "data3 = torch.load(gmm_mlp_path, map_location=device)  # data3 for gmm_mlp\n",
    "\n",
    "model.RGBView1ResnetEmbed.load_state_dict(data1[0])\n",
    "model.RGBView3ResnetEmbed.load_state_dict(data2[1])\n",
    "model.Probot.load_state_dict(data3[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(traj_idx, front_image, agent_image, side_image, eyeinhand_image, eef_pos, eef_quat, gripper_qpos, actions):\n",
    "        front_image = torch.from_numpy(front_images[traj_idx]).permute(2, 0, 1).to(device).float()/255\n",
    "        agent_image = torch.from_numpy(agent_images[traj_idx]).permute(2, 0, 1).to(device).float()/255\n",
    "        side_image = torch.from_numpy(side_images[traj_idx]).permute(2, 0, 1).to(device).float()/255\n",
    "        eyeinhand_image = torch.from_numpy(eyeinhand_images[traj_idx]).permute(2, 0, 1).to(device).float()/255\n",
    "        eef_pos = torch.from_numpy(eef_poses[traj_idx]).to(device).float()\n",
    "        eef_quat = torch.from_numpy(eef_quats[traj_idx]).to(device).float()\n",
    "        gripper_qpos = torch.from_numpy(gripper_qposes[traj_idx]).to(device).float()\n",
    "        action = torch.from_numpy(actions[traj_idx]).to(device).float()\n",
    "\n",
    "        input_args = [eef_pos.unsqueeze(0), \n",
    "                eef_quat.unsqueeze(0), \n",
    "                gripper_qpos.unsqueeze(0), \n",
    "                side_image.unsqueeze(0), \n",
    "        ]\n",
    "        onlyimage_model = Imageonly_Model(model, input_args)\n",
    "\n",
    "        return onlyimage_model, front_image, agent_image, side_image, eyeinhand_image, eef_pos, eef_quat, gripper_qpos, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj_idx in range(len(front_images)):\n",
    "    onlyimage_model, front_image, agent_image, side_image, eyeinhand_image, eef_pos, eef_quat, gripper_qpos, action = get_input(traj_idx, \n",
    "                                                                                                                                 front_images, \n",
    "                                                                                                                                 agent_images, \n",
    "                                                                                                                                 side_images, \n",
    "                                                                                                                                 eyeinhand_images, \n",
    "                                                                                                                                 eef_poses, \n",
    "                                                                                                                                 eef_quats, \n",
    "                                                                                                                                 gripper_qposes,\n",
    "                                                                                                                                 actions)\n",
    "    target_layers = [onlyimage_model.original_model.RGBView3ResnetEmbed.resnet18_base_model.layer4[1].conv2, \n",
    "                 onlyimage_model.original_model.RGBView3ResnetEmbed.resnet18_base_model.layer4[1].conv1,\n",
    "                 onlyimage_model.original_model.RGBView1ResnetEmbed.resnet18_base_model.layer4[1].conv2, \n",
    "                 onlyimage_model.original_model.RGBView1ResnetEmbed.resnet18_base_model.layer4[1].conv1\n",
    "                 ]\n",
    "    cam = GradRAM(model=onlyimage_model, target_layers=target_layers)\n",
    "    input_tensor = eyeinhand_image.unsqueeze(0).to(device).float()\n",
    "\n",
    "    label = action.unsqueeze(0)\n",
    "\n",
    "    targets = [LogProbOutputTarget(label.to(device).float())]\n",
    "\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets, aug_smooth=True, eigen_smooth=True)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    visualization = show_cam_on_image(input_tensor.squeeze(0).permute(1,2,0).cpu().numpy(), grayscale_cam, use_rgb=True)\n",
    "\n",
    "    # # You can also get the model outputs without having to re-inference\n",
    "    model_outputs = cam.outputs\n",
    "    folder = 'results/baseline_demo0'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    Image.fromarray(visualization).save(f'{folder}/gradram{traj_idx}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.lift.ours.resnet18_gmmmlp_view13rgb_rel_model_low_dim_layer import PiNetwork\n",
    "from models.lift.ours.resnet18_gmmmlp_view13rgb_rel_model_low_dim_layer import Imageonly_Model\n",
    "\n",
    "vision1_anchors = np.load(\"/home/generalroboticslab/Desktop/gradcam-view-stitching/models/lift/ours/robot0_eye_in_hand_256anchor_images_from_agentview_idx.npy\")\n",
    "vision2_anchors = np.load(\"/home/generalroboticslab/Desktop/gradcam-view-stitching/models/lift/ours/sideview_256anchor_images_from_agentview_idx.npy\")\n",
    "\n",
    "vision1_anchors_tensor = torch.tensor(vision1_anchors, dtype=torch.float32).to(device).permute(0, 3, 1, 2) / 255.0\n",
    "vision2_anchors_tensor = torch.tensor(vision2_anchors, dtype=torch.float32).to(device).permute(0, 3, 1, 2) / 255.0\n",
    "\n",
    "model = PiNetwork(input_shape, vision1_anchors_tensor, vision2_anchors_tensor, image_latent_dim, action_dim, low_dim_input_dim, mlp_hidden_dims)\n",
    "model.to(device)\n",
    "model.float()\n",
    "\n",
    "# test load and rollout\n",
    "\n",
    "vision1_encoder_path = '/home/generalroboticslab/Desktop/gradcam-view-stitching/models/lift/ours/models/bc_rel_ver1_vision1robot0_eye_in_hand_vision2agentview_anchors256_lr0.0001_seed101_model.pt'\n",
    "vision2_encoder_path = '/home/generalroboticslab/Desktop/gradcam-view-stitching/models/lift/ours/models/bc_rel_ver1_vision1agentview_vision2sideview_anchors256_lr0.0001_seed101_model.pt'\n",
    "gmm_mlp_path = '/home/generalroboticslab/Desktop/gradcam-view-stitching/models/lift/ours/models/bc_rel_ver1_vision1robot0_eye_in_hand_vision2agentview_anchors256_lr0.0001_seed101_model.pt'\n",
    "\n",
    "data1 = torch.load(vision1_encoder_path, map_location=device)  # data1 for task vision1_encoder\n",
    "data2 = torch.load(vision2_encoder_path, map_location=device)  # data2 for vision2_encoder\n",
    "data3 = torch.load(gmm_mlp_path, map_location=device)  # data3 for gmm_mlp\n",
    "\n",
    "model.RGBView1ResnetEmbed.load_state_dict(data1[0])\n",
    "model.RGBView3ResnetEmbed.load_state_dict(data2[1])\n",
    "model.Probot.load_state_dict(data3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(traj_idx, front_image, agent_image, side_image, eyeinhand_image, eef_pos, eef_quat, gripper_qpos, actions):\n",
    "        front_image = torch.from_numpy(front_images[traj_idx]).permute(2, 0, 1).to(device).float()/255\n",
    "        agent_image = torch.from_numpy(agent_images[traj_idx]).permute(2, 0, 1).to(device).float()/255\n",
    "        side_image = torch.from_numpy(side_images[traj_idx]).permute(2, 0, 1).to(device).float()/255\n",
    "        eyeinhand_image = torch.from_numpy(eyeinhand_images[traj_idx]).permute(2, 0, 1).to(device).float()/255\n",
    "        eef_pos = torch.from_numpy(eef_poses[traj_idx]).to(device).float()\n",
    "        eef_quat = torch.from_numpy(eef_quats[traj_idx]).to(device).float()\n",
    "        gripper_qpos = torch.from_numpy(gripper_qposes[traj_idx]).to(device).float()\n",
    "        action = torch.from_numpy(actions[traj_idx]).to(device).float()\n",
    "\n",
    "        input_args = [eef_pos.unsqueeze(0), \n",
    "                eef_quat.unsqueeze(0), \n",
    "                gripper_qpos.unsqueeze(0), \n",
    "                side_image.unsqueeze(0), \n",
    "        ]\n",
    "        onlyimage_model = Imageonly_Model(model, input_args)\n",
    "\n",
    "        return onlyimage_model, front_image, agent_image, side_image, eyeinhand_image, eef_pos, eef_quat, gripper_qpos, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj_idx in range(len(front_images)):\n",
    "    onlyimage_model, front_image, agent_image, side_image, eyeinhand_image, eef_pos, eef_quat, gripper_qpos, action = get_input(traj_idx, \n",
    "                                                                                                                                 front_images, \n",
    "                                                                                                                                 agent_images, \n",
    "                                                                                                                                 side_images, \n",
    "                                                                                                                                 eyeinhand_images, \n",
    "                                                                                                                                 eef_poses, \n",
    "                                                                                                                                 eef_quats, \n",
    "                                                                                                                                 gripper_qposes,\n",
    "                                                                                                                                 actions)\n",
    "    target_layers = [\n",
    "                onlyimage_model.original_model.RGBView3ResnetEmbed.resnet18_base_model.layer4[1].conv2, \n",
    "                 onlyimage_model.original_model.RGBView3ResnetEmbed.resnet18_base_model.layer4[1].conv1,\n",
    "                 onlyimage_model.original_model.RGBView1ResnetEmbed.resnet18_base_model.layer4[1].conv2, \n",
    "                 onlyimage_model.original_model.RGBView1ResnetEmbed.resnet18_base_model.layer4[1].conv1\n",
    "                 ]\n",
    "    cam = GradRAM(model=onlyimage_model, target_layers=target_layers)\n",
    "    input_tensor = eyeinhand_image.unsqueeze(0).to(device).float()\n",
    "\n",
    "    label = action.unsqueeze(0)\n",
    "\n",
    "    targets = [LogProbOutputTarget(label.to(device).float())]\n",
    "\n",
    "    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets,aug_smooth=True, eigen_smooth=True)\n",
    "\n",
    "    # In this example grayscale_cam has only one image in the batch:\n",
    "    # print(grayscale_cam.shape)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    visualization = show_cam_on_image(input_tensor.squeeze(0).permute(1,2,0).cpu().numpy(), grayscale_cam, use_rgb=True)\n",
    "\n",
    "    # # You can also get the model outputs without having to re-inference\n",
    "    model_outputs = cam.outputs\n",
    "    folder = 'results/ours_demo0'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    Image.fromarray(visualization).save(f'{folder}/gradram{traj_idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vid(image_folder, video_path):\n",
    "    import cv2\n",
    "    images = []\n",
    "    length = len(os.listdir(image_folder))\n",
    "    for i in range(length):\n",
    "        filename = f'gradram{i}.png'\n",
    "        path = os.path.join(image_folder, filename)\n",
    "        if os.path.exists(path):\n",
    "            img = cv2.imread(path)\n",
    "            images.append(img)\n",
    "        else:\n",
    "            print(f\"Image {filename} not found. Skipping.\")\n",
    "\n",
    "    if not images:\n",
    "        raise RuntimeError(\"No images found. Check your file paths.\")\n",
    "    \n",
    "    height, width, layers = images[0].shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(video_path, fourcc, 10, (width, height))\n",
    "\n",
    "    for img in images:\n",
    "        video.write(img)\n",
    "\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"/home/generalroboticslab/Desktop/gradcam-view-stitching/results/ours_demo0\"\n",
    "video_path = 'results/video_ours_eye.mp4'  # replace with your desired output path\n",
    "\n",
    "make_vid(image_folder, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"/home/generalroboticslab/Desktop/gradcam-view-stitching/results/baseline_demo0\"\n",
    "video_path = 'results/video_baseline_eye.mp4'  # replace with your desired output path\n",
    "\n",
    "make_vid(image_folder, video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
